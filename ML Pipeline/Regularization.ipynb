{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa0bbc8-1787-44cd-b88e-5972cda7a7ab",
   "metadata": {},
   "source": [
    "### Regularization in Machine Learning:\n",
    "- It is a technique in machine learning used to prevent overfitting by adding a penalty to the model's loss function, encouraging simpler and more generalizable models.\n",
    "- Prevents overfitting - Adds constraints to the model to reduce the risk of memorizing noise in the training data.\n",
    "- Improves generalization - Encourages simpler models that perform better on new, unseen data.\n",
    "  - High variance - Overfitting\n",
    "  - High Bias - Underfitting\n",
    "  - Low Bias & Low variance - Good Balance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e7fdb-bfa4-4e9e-a15b-649c2f8d980b",
   "metadata": {},
   "source": [
    "### Types of regularization:\n",
    "#### 1. LASSO Regularization (or) L1:\n",
    "- LASSO Stands for Least Absolute Shrikage and selection Operator.\n",
    "- It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function(L).\n",
    "- This penalty can shrink some coefficients to zero which helps in selecting only the important features and ignoring the less ones.\n",
    "- Formula, cost = 1/n Σ(y(i) - y^(i))² + α Σ|w(i)|\n",
    "  - where,\n",
    "  - m - No of Features\n",
    "  - n - No of datapoints\n",
    "  - y(i) - Actual Target Value\n",
    "  - y^(i) - Predicted Target Value\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e948d-9260-429a-be7a-41c248a6db07",
   "metadata": {},
   "source": [
    "#### Implementation in Python:\n",
    "- lasso = Lasso(alpha=0.1) - Creates a Lasso regression model with regularization strength alpha set to 0.1.\n",
    "- X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42) - Generates regression dataset with 100 samples, 5 features and some noise.\n",
    "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) - Splits the data into 80% training and 20% testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ec5d81-3255-4e4e-ba7b-c57de0f65beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.06362439921332955\n",
      "Coefficients: [60.50305581 98.52475354 64.3929265  56.96061238 35.52928502]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"MSE:{mse}\")\n",
    "print(\"Coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84076e2-2f9b-4141-9ee7-b104161771a8",
   "metadata": {},
   "source": [
    "#### 2. Ridge Regression (or) L2 Regression:\n",
    "- It adds the square magnitude of the coefficent as a penalty term to the loss function(L).\n",
    "- It handles multicollinearity by shrinking the coefficents of correlated features instead of eliminating them.\n",
    "- Formula, cost = 1/n Σ(y(i) - y^(i))² + α Σ|w²(i)|\n",
    "   - where,\n",
    "   - m - No of Features\n",
    "   - n - No of datapoints\n",
    "   - y(i) - Actual Target Value\n",
    "   - y^(i) - Predicted Target Value\n",
    "   - w(i) - Coefficents of features.\n",
    "   - α - Regularization parameter that controls the strength of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7fb70-8891-4ffc-a507-c3710dbb61b1",
   "metadata": {},
   "source": [
    "### Implementation in Python:\n",
    "- ridge = Ridge(alpha=1.0) - Creates a Ridge regularization model with regularization strength alpha=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fabb89a-16b3-4010-8b8e-838eaa0c59cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.114050771972782\n",
      "Coefficients: [59.87954432 97.15091098 63.24364738 56.31999433 35.34591136]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa86b0f8-80b2-45ac-8855-e9d0e7e737d4",
   "metadata": {},
   "source": [
    "### 3. Elastic Net Regression:\n",
    "- It is a combination of L1 and L2.\n",
    "- That shows that we add the absolute norm of the weights as well as the spread measure of the weights.\n",
    "- With the help an extra hyperparameter that controls the ratio of the L1 and L2 regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92672230-415c-4bdf-bf19-08fe3c238760",
   "metadata": {},
   "source": [
    "### Implemented in Python:\n",
    "- model = ElasticNet(alpha=1.0) - Creates a Elastic Net regression model with regularization strength alpha set to 1.0 and L1/L2 mixing ratio 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fb4262-c21c-495c-878d-9cee56e744a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2662.329268376173\n",
      "Coefficients: [41.2685658  60.6166494  34.45391474 37.4873701  26.29561474]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y =make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\",mse)\n",
    "print(\"Coefficients:\",model.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
