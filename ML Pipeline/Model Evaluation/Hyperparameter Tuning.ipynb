{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728ea421-4eac-440e-a616-40884879b1bb",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning in Machine Learning:\n",
    "- It is the process of selecting the optimal values for a machine learning model's hyperparameters.\n",
    "- These are typically set before the actual training process begins and controls aspects of the learning process itself.\n",
    "- Effective tuning helps the model learn better patterns, avoid overfitting (or) underfitting and achieve higher accuracy on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab69e9e-bea9-4014-b950-cbf59b18a78e",
   "metadata": {},
   "source": [
    "### Techniques for Hyperparameter Tuning:\n",
    "#### 1. Grid Search CV:\n",
    "- It is a brute-force technique for hyperparameter tuning.\n",
    "- It trains the model using all combination of specified hyperparameter values to find the best-performing setup.\n",
    "- It is slow and uses a lot of computer power which make it hard to use with a bigdatasets (or) many settings.\n",
    "- It works using below steps -\n",
    "  1. Create the grid of potential values for each hyperparameter.\n",
    "  2. Train the model for every combination in the grid.\n",
    "  3. Evaluate each model using cross-validation.\n",
    "  4. select the combination that gives a highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb20a22-73b5-4e05-89cf-6dd163795ac8",
   "metadata": {},
   "source": [
    "### Tuning Logistic Regression with GridSearchCV\n",
    "- We generate a sample data using make_classification.\n",
    "- We define a range of C values using logarithmic scale.\n",
    "- GridSearchCV - tries all combinations from param_grid and uses 5-fold cross-validation.\n",
    "- It returns the best hyperparameter (c) and its corresponding validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4b61ed-1a78-4ca0-b6f1-eb8fdd65f95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': np.float64(0.006105402296585327)}\n",
      "Best score is 0.853\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_informative=10, n_classes=2, random_state=42) \n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "print('Tuned Logistic Regression Parameters: {}'.format(logreg_cv.best_params_))\n",
    "print('Best score is {}'.format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fb569-b917-4938-be47-bd064fa9f98e",
   "metadata": {},
   "source": [
    "#### 2. Random Search CV:\n",
    "- It picks random combinations of hyperparameters from the given ranges instead of checking every single combination like GridSearchCV.\n",
    "  1. In each iteration it tries a new random combination of hyperparameter values.\n",
    "  2. It records the model's performance for each combination.\n",
    "  3. After several attempts it selects the best-performing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d259e2-2300-4362-9dc8-196b3ef278e6",
   "metadata": {},
   "source": [
    "### Tuning Decision Tree with RandomizedSearchCV\n",
    "- We define a range of values for each hypermater. E.g - max_depth, min_samples_leaf etc.\n",
    "- Random combinations are picked and evaluated using 5-fold cross-validation.\n",
    "- The best combination and score are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1e624f-79e8-4f71-8c2f-333bbde5ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameter:{'criterion': 'gini', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 8}\n",
      "Best score is 0.805\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
    "                           n_classes=2, random_state=42)\n",
    "param_dist = {\n",
    "    'max_depth': [3, None],\n",
    "    'max_features':randint(1, 9),\n",
    "    'min_samples_leaf': randint(1, 9),\n",
    "    'criterion':['gini', 'entropy']\n",
    "}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "tree_cv.fit(X,y)\n",
    "\n",
    "print('Tuned Decision Tree Parameter:{}'.format(tree_cv.best_params_))\n",
    "print('Best score is {}'.format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab6750-2edd-4469-8213-971cd28b28f1",
   "metadata": {},
   "source": [
    "#### 3. Bayesian Optimization:\n",
    "- GridSearch and Random Search can be inefficient because they blindly try many hyperparameter combination, even if they are clearly not useful.\n",
    "- It treats hyperparameter tuning like a mathematical optimization problem and learns from past results to decide what to try next.\n",
    "  1. Build a probailistic model (surrogate function ) that predicts performance based on parameters.\n",
    "  2. Update this model after each evaluation.\n",
    "  3. Use the model to choose the next best set to try.\n",
    "  4. Repeat until the optimal combination is found. The surrogate function models - P(score(y) \\ hyperparametrs(x))\n",
    "  - Here the surrogate function models the relationshp between the hyperparametrs of x and the score y.\n",
    "  - By updating this model iteratively with each new evaluation Bayesian optimization makes more informed decisions.\n",
    "  - Common surrogate models used in Bayesian optimization include -\n",
    "    1. Gaussian Processes\n",
    "    2. Random Forest Regression\n",
    "    3. Tree-structured Parzen Estimators(TPE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
