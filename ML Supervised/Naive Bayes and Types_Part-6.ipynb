{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef79731c-1103-42c5-a63a-82b6fae915f9",
   "metadata": {},
   "source": [
    "### What is Naive Bayes?\n",
    "- Naive Bayers is a machine learning classification algorithm that predicts based on the category of a data point using probability.\n",
    "- It assumes that all the features are independent each other.\n",
    "- It performs well in real-world applicants such as sentimeny analysis, document categorisation, spam filtering.\n",
    "### Key Features of Naive Bayes Classifiers:\n",
    "- The main idea behind the Naive Bayes Classifer is to use Bayes' Theroem to classify data based on the probabilities of different classes given the features of the data.\n",
    "- It is used mostly high-dimensional text classification.\n",
    "  1. It is a probabilistic classifier because it assumes each feature contribution to the predictions with no relation between each other.\n",
    "  2. It has very few parameters which are used to build ML models that can predict at a faster speed than other classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c006365-557d-415d-942a-99b4a8bd0564",
   "metadata": {},
   "source": [
    "### Assumption of Naive Bayes:\n",
    "- The fundamental Naive Bayes assumption is that each feature makes an:\n",
    "#### 1. Feature Independence:\n",
    "- This means when we are trying to classify something, we assume that each feature in the data does not affect any other feature.\n",
    "#### 2. Continous Features are normally distributed:\n",
    "- If a feature is continous, then it is assumed to be normally distributed within each class.\n",
    "#### 3. Discrete features have multinomial distribution:\n",
    "- If a feature is discrete, then it is assumed to be multinomial distribution within each class.\n",
    "#### 4. Features are equally important:\n",
    "- All features are assumed to contribute equally to the prediction of class label.\n",
    "#### 5. No missing values:\n",
    "- The data should not contains any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4110b2-285d-4c21-9200-ca9ec5ccecd7",
   "metadata": {},
   "source": [
    "### Introduction to Bayes's Theorem:\n",
    "- Bayes'Theorem provides a principled way to reverse conditional probailities.\n",
    "It is defined as -\n",
    "- Formula, p(y/X) = p(X/y).p(y) / p(X)\n",
    "  - where -\n",
    "    - p(y/X) - Posbterior probaility, probability of class y given features X.\n",
    "    - p(X/y) - Likelihood, probability of features X given class y.\n",
    "    - p(y) - Prior probability of class y.\n",
    "    - p(X) - Marginal likelihood (or) evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8f894-fbb4-4fd2-a687-c2552b3ea55d",
   "metadata": {},
   "source": [
    "### Naive Bayes Working:\n",
    "#### 1. Terminology:\n",
    "- Consider a classification problem.Then:\n",
    "  1. y is the class label (e.g - \"yes\" (or) \"no\")\n",
    "  2. X = (x1, x2, x3, ...,xn) is the feature vector(rows).\n",
    "\n",
    "#### 2. The Naive Assumption:\n",
    "- The \"naive\" in Naive Bayes comes from the assumption that all features are independent given the class.That is -\n",
    "   - p(x1, x2,....,xn/y) = p(x1/y).p(x2/y)....p(xn/y)\n",
    "-Thus, Bayes'theroem becomes -\n",
    " - p(y/x1, x2,... xn) = p(y).p(x(i)/y) / p(X)\n",
    "- Since denominator is constant for a given input, we can write:\n",
    "  - p(y/x1,x2,.....xn) ‚àù p(y).p(x(i)/y)\n",
    "#### 3. Constructing the Naive Bayes Classifier:\n",
    "- We compute the posterier for each class y and choose the class with the highest probability -\n",
    "  - y^ = arg max(y) p(y).p(x(i)/y)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2148ce-d6de-45f1-a675-0b32601390dc",
   "metadata": {},
   "source": [
    "### Types of Naivo Bayes Model:\n",
    "- There are three types of Navie Bayes Model -\n",
    "  #### 1. Gaussian Naive Bayes:\n",
    "  - It is also called as Normal distribution which plotted, it gives a bell shaped curve which is symmetric about the mean of the feature values as shown below.\n",
    "  - It continous values assoicated with each feature are assumed to be distributed according to a Gaussian Distrubution.\n",
    "  #### 2. Multinomial Naive Bayes:\n",
    "  - It is used when features are represent the frequency of terms in a document.\n",
    "  - It is commonly applied in text classification, where term frequencies are important.\n",
    "  #### 3. Bernoulli Navie Bayes:\n",
    "  - It deals with binary features, where each feature indicates wheather a word appears (or) not in a document.\n",
    "  - It is suited for scenarios where the presence (or) absence of terms is more relevant than their frequency.\n",
    "  - Both models are widely used in document classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14ad29-f6c3-4f69-9156-fd4e5a3a91e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
