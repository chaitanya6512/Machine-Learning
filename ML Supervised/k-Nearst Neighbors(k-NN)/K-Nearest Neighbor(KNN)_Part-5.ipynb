{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d48ab80-29ff-4fbd-b7fc-3d1169fad2a3",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour (KNN) Algorithm\n",
    "- KNN is a supervised machine learning algorithm generally used for classification but can also be used for regression tasks.\n",
    "- It works by finding the \"k\" closest data points(neighbous) to a given input and makes a predictions based on the majority class(for classification) (or) the average value (for regression).\n",
    "- It is also called as a lazy learner algorithm because it does not learn from the training set immediately instead it stores the entire dataset and performs computations only at the time of classification.\n",
    "- Ex - Imagine you're deciding which fruit it is based on its shape and size.You compare it to fruits you already know.\n",
    "  - If k=3, the algorithm looks at the 3 closest fruits to the new one.\n",
    "  - If 2 of those 3 fruits are apples and 1 is a banana, the algorithm says the new fruit is an apple because most of its neighbours are apples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0afb5-090a-48f0-834c-3f99d8f3dbce",
   "metadata": {},
   "source": [
    "### How to choose the value of k for KNN Algorithm?\n",
    "- The value of k in KNN decides how many neighbors the algorithm looks at when making a prediction.\n",
    "- Choosing the right k is important for good results.\n",
    "- If the data has lots of noise (or) outliers, using a larger k can be make the predictions more stable.\n",
    "- But if k is too large the model may become too simple and miss important patterns and this is called underfitting.\n",
    "- So k should be picked carefully based on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435172b2-63c8-4ca1-8e14-e7da80a64e5e",
   "metadata": {},
   "source": [
    "### Statistical Methods for Selecting k\n",
    "#### 1. Cross-Validation:\n",
    "- It is a good way to find the best value of \"k\" is by using k-fold cross-validation.This means dividing the dataset into \"k\" parts.\n",
    "- The model is trained on some of these parts and tested on remaining ones. This process is repeated for each part.\n",
    "- The \"k\" value that gives the highest average accuracy during these tests is usually the best one to use.\n",
    "#### 2. Elbow Method:\n",
    "- In Elbow Method, we draw graph showing the error rate(or) accuracy for different k values.\n",
    "- As \"k\" increases the error usually drops first. But after a certain point error stops decreasing quickly.\n",
    "- The point where curve changes direction and looks like an \"elbow\" is usually the best choice for \"k\".\n",
    "#### 3. Odd Values for k:\n",
    "- It's good idea to use an odd number for \"k\" especially in classification problems.\n",
    "- This helps avoid ties when deciding which class is the most common among the neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47895b8-7a43-4c09-ab51-af12a7426f66",
   "metadata": {},
   "source": [
    "### Distance metrics Used in KNN algorithm:\n",
    "- KNN uses distance metrics to identify nearst neighbor,these neighbors are used for classification and regression tasks.\n",
    "- To identify nerast neighbor we use below distance metrics:\n",
    "#### 1. Euclidean Distance:\n",
    "- It is defined as straight-line between two points.\n",
    "- Formula, d = √(x2-x1)2+(y2-y1)2\n",
    " \n",
    "#### 2. Manhatten Distance:\n",
    "- It is a metric used to calculate the distance between two points in a grid-like space by summing the absolute differences of their coordinates across all dimensions.\n",
    "- It is also called as \"taxicab distance\".\n",
    "- Formula, d(x,y) = Σ|x(i) - y(i)|\n",
    "#### 3. Minikowski Distance:\n",
    "- It is like a family of distances, which includes both Euclidean and Manhatten distances as special cases:\n",
    "- Formula, d(x,y) = (Σ(x)(i)-y(i))p)1/p\n",
    "- From the formula above,\n",
    "  - when p=2, it becomes the same as the Euclidean distance formula.\n",
    "  - when p=1, it turns into the Manhatten distance formula.\n",
    "- Minkowski distance is essentially a flexible formula that can represent either Euclidean (or) Manhatten distance depending on the value of p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da68e2-0a47-4193-84ba-f422dcf83f9d",
   "metadata": {},
   "source": [
    "### Implementing KNN from Scratch in Python\n",
    "#### 1. Importing Libraries:\n",
    "- Counter is used to count the occurances of elements in a list (or) iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63c7ca9-5217-4f1d-9ca9-37b45347da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c8c2e-f7b1-4a49-b308-d3acc97d3dc9",
   "metadata": {},
   "source": [
    "#### 2. Defining the Euclidean Distance Function\n",
    "- euclidean_distance - is to calculate euclidean distance between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5f4cd8-94e1-4940-b079-2acada00714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((np.array(point1) - np.array(point2))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4996e2d-39fd-4627-a2a7-1f8648336c62",
   "metadata": {},
   "source": [
    "#### 3. KNN Prediction Function\n",
    "- distance.append - It saves how far each training point is from the test point, along with its label.\n",
    "- distances.sort - It is used to sorts the list so the nearst points come first.\n",
    "- k_nearst_labels - picks the labels of the \"k\" closest points.\n",
    "- Uses Counter to find which label apperas most among those \"k\" labels that becomes the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93e5a1e-6df1-4132-b9f4-8f1b7598ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(training_data, training_labels, test_point, k):\n",
    "    distances = []\n",
    "    for i in range(len(training_data)):\n",
    "        dist = euclidean_distance(test_point, training_data[i])\n",
    "        distances.append((dist, training_labels[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearst_labels = [label for _, label in distances[:k]]\n",
    "    return Counter(k_nearst_labels).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ddda3b-84be-49cd-a05d-afd865e27acc",
   "metadata": {},
   "source": [
    "#### 4. Training Data, Labels and Test Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0b3119-f2d5-4d14-ae04-b452c8bd13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[1,2], [2,3], [3,4], [6,7], [7,8]]\n",
    "training_labels = ['A', 'A', 'A', 'B', 'B']\n",
    "test_point = [4,5]\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed7b71-9631-4c06-ad59-c32d91ff74b3",
   "metadata": {},
   "source": [
    "#### 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cab31ed-531e-4e8a-933b-6a0c9003e27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "prediction = knn_predict(training_data, training_labels, test_point, k)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b9a45-c2d2-4391-9acd-ed77609e1dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
